{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load JSON data\n",
    "with open('new.json') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract patterns and corresponding intents\n",
    "patterns = []\n",
    "intents = []\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        patterns.append(pattern)\n",
    "        intents.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode intents\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_intents = label_encoder.fit_transform(intents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "train_patterns, val_patterns, train_intents, val_intents = train_test_split(patterns, encoded_intents, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize patterns\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_encodings = tokenizer(train_patterns, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_patterns, truncation=True, padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "class IntentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = IntentDataset(train_encodings, train_intents)\n",
    "val_dataset = IntentDataset(val_encodings, val_intents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/20:  36%|███▌      | 10/28 [00:29<00:49,  2.77s/it]"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(set(intents)))\n",
    "\n",
    "# Fine-tune the model\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "model.train()\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{epochs}\"):\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print(f\"Epoch {epoch+1} Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        total_correct += (predictions == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "accuracy = total_correct / total_samples\n",
    "print(f'Validation accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fine_tuned_bert_2\\\\tokenizer_config.json',\n",
       " 'fine_tuned_bert_2\\\\special_tokens_map.json',\n",
       " 'fine_tuned_bert_2\\\\vocab.txt',\n",
       " 'fine_tuned_bert_2\\\\added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.save_pretrained(\"fine_tuned_bert_2\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_bert_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('fine_tuned_bert', num_labels=len(set(intents)))\n",
    "tokenizer = BertTokenizer.from_pretrained('fine_tuned_bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question):\n",
    "    encoded_question = tokenizer(question, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    outputs = model(**encoded_question)\n",
    "    predicted_label = torch.argmax(outputs.logits[0]).item()\n",
    "    predicted_intent = label_encoder.inverse_transform([predicted_label])[0]\n",
    "\n",
    "    # Find the intent from the JSON data\n",
    "    for intent in data['intents']:\n",
    "        if intent['tag'] == predicted_intent:\n",
    "            # Randomly select a response from the list of responses\n",
    "            response = intent['responses']\n",
    "            return response\n",
    "        else:\n",
    "    \n",
    "            return \"Sorry, I couldn't understand your question.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, I couldn't understand your question.\n"
     ]
    }
   ],
   "source": [
    "msg = \"Contact Details\"\n",
    "response =ask_question(msg)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 5.53101921081543\n",
      "Epoch 2/20, Loss: 5.060510635375977\n",
      "Epoch 3/20, Loss: 4.627333641052246\n",
      "Epoch 4/20, Loss: 4.280425071716309\n",
      "Epoch 5/20, Loss: 4.033732891082764\n",
      "Epoch 6/20, Loss: 3.6480495929718018\n",
      "Epoch 7/20, Loss: 3.3868675231933594\n",
      "Epoch 8/20, Loss: 3.086242437362671\n",
      "Epoch 9/20, Loss: 2.949183940887451\n",
      "Epoch 10/20, Loss: 2.6545653343200684\n",
      "Epoch 11/20, Loss: 2.5983669757843018\n",
      "Epoch 12/20, Loss: 2.2550389766693115\n",
      "Epoch 13/20, Loss: 2.0575857162475586\n",
      "Epoch 14/20, Loss: 1.8628835678100586\n",
      "Epoch 15/20, Loss: 1.613502025604248\n",
      "Epoch 16/20, Loss: 1.5146889686584473\n",
      "Epoch 17/20, Loss: 1.30256986618042\n",
      "Epoch 18/20, Loss: 1.1838409900665283\n",
      "Epoch 19/20, Loss: 0.9983706474304199\n",
      "Epoch 20/20, Loss: 0.879201352596283\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "with open('new_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract text from JSON data\n",
    "training_text = \"\"\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        training_text += pattern + \" \"\n",
    "\n",
    "# Ensure the text is converted to lowercase\n",
    "training_text = training_text.lower()\n",
    "\n",
    "\n",
    "\n",
    "# Fine-tune GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "max_seq_length = 1024\n",
    "\n",
    "# Check if training_text exceeds max_seq_length\n",
    "if len(training_text) > max_seq_length:\n",
    "    training_text = training_text[:max_seq_length]\n",
    "\n",
    "# Convert the text to lowercase\n",
    "training_text = training_text.lower()\n",
    "\n",
    "# Tokenize the text\n",
    "input_ids = tokenizer.encode(training_text, return_tensors='pt')\n",
    "\n",
    "# Truncate input_ids if it exceeds the model's maximum sequence length\n",
    "if input_ids.size()[1] > tokenizer.model_max_length:\n",
    "    input_ids = input_ids[:, :tokenizer.model_max_length]\n",
    "\n",
    "# Create attention mask\n",
    "attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
    "\n",
    "# Model training loop\n",
    "model.train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(20):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{20}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fine_tuned_gpt2\\\\tokenizer_config.json',\n",
       " 'fine_tuned_gpt2\\\\special_tokens_map.json',\n",
       " 'fine_tuned_gpt2\\\\vocab.json',\n",
       " 'fine_tuned_gpt2\\\\merges.txt',\n",
       " 'fine_tuned_gpt2\\\\added_tokens.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"fine_tuned_gpt2\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "# from flask import Flask, render_template, request\n",
    "\n",
    "# app = Flask(__name__)\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"fine_tuned_gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"fine_tuned_gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Function to generate response using GPT-2\n",
    "def generate_response(prompt, max_length=50):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=max_length, truncation=True)\n",
    "    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# @app.route(\"/\")\n",
    "# def index():\n",
    "#     return render_template('chat.html')\n",
    "\n",
    "# @app.route(\"/get\", methods=[\"GET\", \"POST\"])\n",
    "# def chat():\n",
    "#     msg = request.form[\"msg\"]\n",
    "#     response = generate_response(msg)\n",
    "#     return response\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Property in New delhi\" and \"name\" in the same file.\n",
      "\n",
      "In the example above, the \"name\" is the name of the property in the property description.\n",
      "\n",
      "In the example above, the \"name\" is the\n"
     ]
    }
   ],
   "source": [
    "msg = \"Property in New delhi\"\n",
    "response =generate_response(msg)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
